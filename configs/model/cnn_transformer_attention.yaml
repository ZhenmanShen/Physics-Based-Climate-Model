# @package _global_.model

type: cnn_transformer_attention

# Number of filters in CNN encoder output (also the transformer embedding dimension)
embed_dim: 128

# Number of transformer layers
depth: 4

# Number of attention heads in transformer
n_heads: 4

# Size of feedforward network in transformer
mlp_dim: 256

# Dropout in transformer layers
dropout: 0.1