# @package _global_.training

lr: 1.0e-4  # Reduced learning rate

# Optimizer settings
optimizer_name: "adamw" # Options: "adam", "adamw"
weight_decay: 0.0001    # Weight decay for AdamW 