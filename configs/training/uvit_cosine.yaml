# @package _global_.training

lr: 5e-4

optimizer:
  _target_: torch.optim.AdamW
  lr: ${training.lr}  
  weight_decay: 1.0e-4

# 5-epoch warm-up is handled inside Model.configure_optimizers()
# by stepping the scheduler only **after** warm-up, so here we just
# declare the cosine cycle (one full cycle = 100 epochs)
scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
  T_0: 100        # first (and only) restart after 100 epochs
  T_mult: 1
  eta_min: 2.5e-5        # 5% of initial lr
