{"format": "torch", "nodes": [{"name": "encoder.0", "id": 140164955611344, "class_name": "Conv2d(5, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))", "parameters": [["weight", [64, 5, 3, 3]], ["bias", [64]]], "output_shape": [[64, 64, 24, 36]], "num_parameters": [2880, 64]}, {"name": "encoder.1", "id": 140164961323280, "class_name": "ReLU()", "parameters": [], "output_shape": [[64, 64, 24, 36]], "num_parameters": []}, {"name": "encoder.2", "id": 140164954699792, "class_name": "Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))", "parameters": [["weight", [128, 64, 3, 3]], ["bias", [128]]], "output_shape": [[64, 128, 12, 18]], "num_parameters": [73728, 128]}, {"name": "encoder.3", "id": 140164954695184, "class_name": "ReLU()", "parameters": [], "output_shape": [[64, 128, 12, 18]], "num_parameters": []}, {"name": "cbam", "id": 140164955608656, "class_name": "CBAM(\n  (avg_pool): AdaptiveAvgPool2d(output_size=1)\n  (max_pool): AdaptiveMaxPool2d(output_size=1)\n  (mlp): Sequential(\n    (0): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): ReLU()\n    (2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  )\n  (sigmoid_channel): Sigmoid()\n  (conv_spatial): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n  (sigmoid_spatial): Sigmoid()\n)", "parameters": [["mlp.0.weight", [8, 128, 1, 1]], ["mlp.2.weight", [128, 8, 1, 1]], ["conv_spatial.weight", [1, 2, 7, 7]]], "output_shape": [[64, 128, 12, 18]], "num_parameters": [1024, 1024, 98]}, {"name": "transformer", "id": 140164954789968, "class_name": "TransformerEncoder(\n  (layers): ModuleList(\n    (0-3): 4 x TransformerEncoderLayer(\n      (self_attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n      )\n      (linear1): Linear(in_features=128, out_features=256, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (linear2): Linear(in_features=256, out_features=128, bias=True)\n      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (dropout1): Dropout(p=0.1, inplace=False)\n      (dropout2): Dropout(p=0.1, inplace=False)\n    )\n  )\n)", "parameters": [["layers.0.self_attn.in_proj_weight", [384, 128]], ["layers.0.self_attn.in_proj_bias", [384]], ["layers.0.self_attn.out_proj.weight", [128, 128]], ["layers.0.self_attn.out_proj.bias", [128]], ["layers.0.linear1.weight", [256, 128]], ["layers.0.linear1.bias", [256]], ["layers.0.linear2.weight", [128, 256]], ["layers.0.linear2.bias", [128]], ["layers.0.norm1.weight", [128]], ["layers.0.norm1.bias", [128]], ["layers.0.norm2.weight", [128]], ["layers.0.norm2.bias", [128]], ["layers.1.self_attn.in_proj_weight", [384, 128]], ["layers.1.self_attn.in_proj_bias", [384]], ["layers.1.self_attn.out_proj.weight", [128, 128]], ["layers.1.self_attn.out_proj.bias", [128]], ["layers.1.linear1.weight", [256, 128]], ["layers.1.linear1.bias", [256]], ["layers.1.linear2.weight", [128, 256]], ["layers.1.linear2.bias", [128]], ["layers.1.norm1.weight", [128]], ["layers.1.norm1.bias", [128]], ["layers.1.norm2.weight", [128]], ["layers.1.norm2.bias", [128]], ["layers.2.self_attn.in_proj_weight", [384, 128]], ["layers.2.self_attn.in_proj_bias", [384]], ["layers.2.self_attn.out_proj.weight", [128, 128]], ["layers.2.self_attn.out_proj.bias", [128]], ["layers.2.linear1.weight", [256, 128]], ["layers.2.linear1.bias", [256]], ["layers.2.linear2.weight", [128, 256]], ["layers.2.linear2.bias", [128]], ["layers.2.norm1.weight", [128]], ["layers.2.norm1.bias", [128]], ["layers.2.norm2.weight", [128]], ["layers.2.norm2.bias", [128]], ["layers.3.self_attn.in_proj_weight", [384, 128]], ["layers.3.self_attn.in_proj_bias", [384]], ["layers.3.self_attn.out_proj.weight", [128, 128]], ["layers.3.self_attn.out_proj.bias", [128]], ["layers.3.linear1.weight", [256, 128]], ["layers.3.linear1.bias", [256]], ["layers.3.linear2.weight", [128, 256]], ["layers.3.linear2.bias", [128]], ["layers.3.norm1.weight", [128]], ["layers.3.norm1.bias", [128]], ["layers.3.norm2.weight", [128]], ["layers.3.norm2.bias", [128]]], "output_shape": [[64, 216, 128]], "num_parameters": [49152, 384, 16384, 128, 32768, 256, 32768, 128, 128, 128, 128, 128, 49152, 384, 16384, 128, 32768, 256, 32768, 128, 128, 128, 128, 128, 49152, 384, 16384, 128, 32768, 256, 32768, 128, 128, 128, 128, 128, 49152, 384, 16384, 128, 32768, 256, 32768, 128, 128, 128, 128, 128]}, {"name": "decoder.0", "id": 140164955605712, "class_name": "ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))", "parameters": [["weight", [128, 64, 2, 2]], ["bias", [64]]], "output_shape": [[64, 64, 24, 36]], "num_parameters": [32768, 64]}, {"name": "decoder.1", "id": 140164954044048, "class_name": "ReLU()", "parameters": [], "output_shape": [[64, 64, 24, 36]], "num_parameters": []}, {"name": "decoder.2", "id": 140164954040848, "class_name": "ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))", "parameters": [["weight", [64, 32, 2, 2]], ["bias", [32]]], "output_shape": [[64, 32, 48, 72]], "num_parameters": [8192, 32]}, {"name": "decoder.3", "id": 140164953126992, "class_name": "ReLU()", "parameters": [], "output_shape": [[64, 32, 48, 72]], "num_parameters": []}, {"name": "decoder.4", "id": 140164953127248, "class_name": "Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))", "parameters": [["weight", [2, 32, 1, 1]], ["bias", [2]]], "output_shape": [[64, 2, 48, 72]], "num_parameters": [64, 2]}], "edges": []}